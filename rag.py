# rag.py
from langchain_core.globals import set_verbose, set_debug
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain.schema.output_parser import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader  # ì´ ë¶€ë¶„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ PDF ê´€ë ¨ ì˜ì¡´ì„± í™•ì¸ í•„ìš”
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_core.prompts import ChatPromptTemplate
import logging

# ë””ë²„ê¹… ë° ë¡œê¹… ì„¤ì •
set_debug(True)
set_verbose(True)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChatPDF:
    """PDFë¥¼ ì²˜ë¦¬í•˜ê³  RAG ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, llm_model: str = "deepseek-r1:latest", embedding_model: str = "mxbai-embed-large"):
        """
        LLMê³¼ ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.model = ChatOllama(model=llm_model)  # ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OllamaEmbeddings(model=embedding_model)  # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)  # ë¬¸ì„œ ë¶„í•  ì„¤ì •

        # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt = ChatPromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µí•´ì£¼ì„¸ìš”.
            ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.

            ë¬¸ë§¥:
            {context}
            
            ì§ˆë¬¸:
            {question}
            
            ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            """
        )

        self.vector_store = None  # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.retriever = None     # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”

    def ingest(self, pdf_file_path: str) -> dict:
        """
        PDF íŒŒì¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ì†Œì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        try:
            logger.info(f"[ingest ì‹œì‘] íŒŒì¼ ê²½ë¡œ: {pdf_file_path}")
            docs = PyPDFLoader(file_path=pdf_file_path).load()  # PDF ë¬¸ì„œ ë¡œë”©
            chunks = self.text_splitter.split_documents(docs)  # ë¬¸ì„œ ë¶„í• 
            chunks = filter_complex_metadata(chunks)  # ë³µì¡í•œ ë©”íƒ€ë°ì´í„° í•„í„°ë§
            
            logger.info(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±ë¨")
            logger.info(f"ìƒ˜í”Œ ì²­í¬ ë‚´ìš©: {chunks[0].page_content[:100]}...")

            self.vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
            )

            # ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜ í™•ì¸
            collection_size = self.vector_store._collection.count()
            logger.info(f"ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜: {collection_size}")

            logger.info("ingest ì™„ë£Œ")
            return {"status": "success", "message": f"ì„±ê³µì ìœ¼ë¡œ {len(chunks)}ê°œì˜ ë¬¸ì„œë¥¼ ì¸ì œìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤."}
        except Exception as e:
            logger.error(f"[ingest ì˜¤ë¥˜] {str(e)}")
            return {"status": "error", "message": str(e)}

    def ask(self, query: str, k: int = 5, score_threshold: float = 0.2):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not self.vector_store:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì¸ì œìŠ¤íŠ¸í•˜ì„¸ìš”.")

        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if not self.retriever:
            self.retriever = self.vector_store.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"k": k, "score_threshold": score_threshold},
            )

        logger.info(f"ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ë§¥ ê²€ìƒ‰: {query}")
        retrieved_docs = self.retriever.invoke(query)  # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰

        if not retrieved_docs:
            return "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ë§¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì›ë¬¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì´ {len(retrieved_docs)}ê°œì˜ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for i, doc in enumerate(retrieved_docs):
            logger.info(f"[ë¬¸ì„œ {i+1}] {doc.page_content[:500]}...")  # 500ìê¹Œì§€ë§Œ ì¶œë ¥ (í•„ìš” ì‹œ ì¡°ì ˆ)

        # LLM ì…ë ¥ í¬ë§· ìƒì„±
        formatted_input = {
            "context": "\n\n".join(doc.page_content for doc in retrieved_docs),
            "question": query,
        }

        # RAG ì²´ì¸ êµ¬ì„±
        chain = (
            RunnablePassthrough()
            | self.prompt
            | self.model
            | StrOutputParser()
        )

        logger.info("LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± ì¤‘.")
        return chain.invoke(formatted_input)


    def clear(self):
        """
        ë²¡í„° ì €ì¥ì†Œì™€ ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        logger.info("ë²¡í„° ì €ì¥ì†Œ ë° ê²€ìƒ‰ê¸° ì´ˆê¸°í™”ë¨")
        self.vector_store = None
        self.retriever = None
